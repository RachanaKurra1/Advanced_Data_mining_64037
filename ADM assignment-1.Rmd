---
title: "ADM Assignment-1"
author: "Rachana Kurra"
date: "2023-03-03"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


 
PART-A 
 
 
QA1. What is the main purpose of regularization when training predictive models? 

Generally, Regularization methods are used in machine learning to avoid underfitting or overfitting for unseen data. Regularization strives to optimize the training set's performance to prevent underfitting, but it also penalizes the model when it gets complicated which is known as overfitting and generalizes well i.e., it improves a modelâ€™s performance by simplifying it.  
Techniques: Lasso and Ridge Regressions were used for regularization.

QA2. What is the role of a loss function in a predictive model? And name two common loss functions for regression models and two common loss functions for classification models

The role of the loss function can define how good a prediction is by measuring the penalty. It generally commutes the distance between the current output and the expected output of the algorithm. The process of reducing the loss is achieved by changing the parameters until the lowest loss is encountered.
Common loss functions for regression models are: MSE (mean squared error) and MAE(mean absolute error)
And for classification models: Binary cross-entropy and categorical cross-entropy loss.


QA3. Consider the following scenario. You are building a classification model with many hyper parameters on a relatively small dataset. You will see that the training error is extremely small. Can you fully trust this model? Discuss the reason.


If the errors in the training set are extremely small, it means that the model is overfitting and it has all the noise data and outliers.  Considering the given example, the model with many hyperparameters is built using a small data set. So, there is a high chance of overfitting. When overfitting occurs, the model may achieve good training accuracy, but it will perform poorly on unseen data. Hence, to address this issue, we need to evaluate the model on a validation set or using cross-validation techniques, which will provide an estimate of the model's generalization performance. 
If the model performs well on the validation set and the variance is less between train error and validation error then it might be trustworthy or reliable.


QA4. What is the role of the lambda parameter in regularized linear models such as Lasso or 
Ridge regression models? 


The lambda parameter is also known as the regularization parameter/ hyperparameter for regularized linear models which prevents overfitting by adding a penalty to the cost function that the model is trying to minimize. The tuning parameter lambda is chosen by cross validation. Increasing or decreasing lambda provides a tradeoff between underfitting and overfitting.
In lasso, the penalty has the effect of forcing coefficients to zero, the penalty is the sum of the absolute values of the coefficients 
In ridge, when the lambda increases, the penalty grows the coefficients will get close to zero but non-zero coefficients. The shrinkage penalty is lambda times the sum of squares of the coefficients so coefficients that get too large are penalized.


 PART-B

```{r}
#loading the libraries
library(ISLR)
library(dplyr)
library(glmnet)
library(caret)
summary(Carseats)
```

```{r}
#Cosidering the input attributes and scaling them and also coverting them to matrix formart 

Carseats_Filtered = Carseats %>%  select ( "Price", 
"Advertising","Population","Age","Income","Education")%>% scale(center = TRUE, scale = TRUE) %>% as.matrix() 
```

```{r}

#storing input attributes and response variable into X,Y in matrix formats 
X = (Carseats_Filtered)
Y = Carseats %>% select("Sales") %>% as.matrix()

```

```{r}
# Question-1: apply lasso regression model on the data to predict sales based on all other attributes.

Model = glmnet(X, Y)
summary(Model)
plot(Model)
```

```{r}
#Performing  k-fold cross-validation to find optimal lambda value
cv_Model <- cv.glmnet(X, Y, alpha = 1)

#finding optimal lambda value that minimizes test MSE
best_lambda <- cv_Model$lambda.min
best_lambda

#plot of test MSE by lambda value
plot(cv_Model) 

```
***From the above results, we can say that increase of variance in the dependent variable (sales) is explanied by regularization for the given attributes and we got the best lambda as 0.004305***

```{r}
#question-2: coefficient for the price attribute in the best model
cv_Model$lambda.1se
coef(cv_Model, s= "lambda.min")

```
***The coefficient of price attribute is -1.35 for the best value of lambda***

```{r}
#QUESTION-3: No. of attributes remaning in the model if lamda is set to 0.01,and what is lambda if it increases to 0.1.

# when lamda sets to 0.01

coef(cv_Model, s=0.01)

#Lambda when set to 0.1.

coef(cv_Model, s=0.1)

#lambda when set to 0.2

coef(cv_Model, s=0.2)
```
***From the results, we can observe that change in lambda value to 0.01 doesn't make any difference with the coefficients but the  2 coefficients are forced to 0 if increase the lambda value to 0.1 and 0.2. Hence, we can say that increase in lambda value will shrink the coefficients to zero.***

```{r}
#question-4: building elastic-net model with alpha set to 0.6

fit.elnet= glmnet(X,Y, alpha=0.6)
plot(fit.elnet, xvar="lambda")
plot(cv.glmnet(X,Y, alpha=0.6))
```

```{r}
fit.elnet <- cv.glmnet(X, Y, alpha = 0.6)

#optimal lambda value
bestlambda <- fit.elnet$lambda.min
bestlambda

print(fit.elnet)

summary(fit.elnet)
```
***Best lambda value  for elastic-net model is 0.00654***


